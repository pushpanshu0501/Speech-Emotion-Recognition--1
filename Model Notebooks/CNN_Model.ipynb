{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6 CNN Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mdxwpf0bGdq0"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import IPython.display as ipd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_nUFvHQGnsV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "83285d47-4dc1-441d-b26c-af53148931b7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "Features = pd.read_csv('/content/drive/MyDrive/features_dataset.csv')\n",
        "Features.head()\n",
        "# Features.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "      <th>128</th>\n",
              "      <th>129</th>\n",
              "      <th>130</th>\n",
              "      <th>131</th>\n",
              "      <th>132</th>\n",
              "      <th>133</th>\n",
              "      <th>134</th>\n",
              "      <th>135</th>\n",
              "      <th>136</th>\n",
              "      <th>137</th>\n",
              "      <th>138</th>\n",
              "      <th>139</th>\n",
              "      <th>140</th>\n",
              "      <th>141</th>\n",
              "      <th>142</th>\n",
              "      <th>143</th>\n",
              "      <th>144</th>\n",
              "      <th>145</th>\n",
              "      <th>146</th>\n",
              "      <th>147</th>\n",
              "      <th>148</th>\n",
              "      <th>149</th>\n",
              "      <th>150</th>\n",
              "      <th>151</th>\n",
              "      <th>152</th>\n",
              "      <th>153</th>\n",
              "      <th>154</th>\n",
              "      <th>155</th>\n",
              "      <th>156</th>\n",
              "      <th>157</th>\n",
              "      <th>158</th>\n",
              "      <th>159</th>\n",
              "      <th>160</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.272267</td>\n",
              "      <td>0.689451</td>\n",
              "      <td>0.708028</td>\n",
              "      <td>0.666473</td>\n",
              "      <td>0.715468</td>\n",
              "      <td>0.694820</td>\n",
              "      <td>0.627661</td>\n",
              "      <td>0.632560</td>\n",
              "      <td>0.687715</td>\n",
              "      <td>0.712157</td>\n",
              "      <td>0.706116</td>\n",
              "      <td>0.696561</td>\n",
              "      <td>0.666424</td>\n",
              "      <td>-505.009247</td>\n",
              "      <td>64.000099</td>\n",
              "      <td>-2.749660</td>\n",
              "      <td>16.950371</td>\n",
              "      <td>-1.089467</td>\n",
              "      <td>-2.046432</td>\n",
              "      <td>-7.829981</td>\n",
              "      <td>-8.716751</td>\n",
              "      <td>-19.273317</td>\n",
              "      <td>-5.294091</td>\n",
              "      <td>-5.584455</td>\n",
              "      <td>-5.783628</td>\n",
              "      <td>-1.870991</td>\n",
              "      <td>-7.146638</td>\n",
              "      <td>-3.675263</td>\n",
              "      <td>-0.451763</td>\n",
              "      <td>-11.253410</td>\n",
              "      <td>-3.521277</td>\n",
              "      <td>-3.482842</td>\n",
              "      <td>-5.802355</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.000173</td>\n",
              "      <td>0.001309</td>\n",
              "      <td>0.018095</td>\n",
              "      <td>0.134511</td>\n",
              "      <td>0.229614</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000877</td>\n",
              "      <td>0.001587</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001960</td>\n",
              "      <td>0.001981</td>\n",
              "      <td>0.002306</td>\n",
              "      <td>0.001958</td>\n",
              "      <td>0.002031</td>\n",
              "      <td>0.001465</td>\n",
              "      <td>0.001945</td>\n",
              "      <td>0.003857</td>\n",
              "      <td>0.003231</td>\n",
              "      <td>0.002353</td>\n",
              "      <td>0.003053</td>\n",
              "      <td>0.002795</td>\n",
              "      <td>0.002131</td>\n",
              "      <td>0.001839</td>\n",
              "      <td>0.001425</td>\n",
              "      <td>0.000817</td>\n",
              "      <td>0.000714</td>\n",
              "      <td>0.000658</td>\n",
              "      <td>0.001076</td>\n",
              "      <td>0.001073</td>\n",
              "      <td>0.001062</td>\n",
              "      <td>0.000639</td>\n",
              "      <td>0.000917</td>\n",
              "      <td>0.001054</td>\n",
              "      <td>0.001587</td>\n",
              "      <td>0.001744</td>\n",
              "      <td>0.001006</td>\n",
              "      <td>0.000687</td>\n",
              "      <td>0.000502</td>\n",
              "      <td>0.000372</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.000288</td>\n",
              "      <td>0.000349</td>\n",
              "      <td>0.000143</td>\n",
              "      <td>1.498768e-05</td>\n",
              "      <td>angry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.262035</td>\n",
              "      <td>0.603476</td>\n",
              "      <td>0.668302</td>\n",
              "      <td>0.692199</td>\n",
              "      <td>0.709884</td>\n",
              "      <td>0.658301</td>\n",
              "      <td>0.605176</td>\n",
              "      <td>0.609343</td>\n",
              "      <td>0.640842</td>\n",
              "      <td>0.689348</td>\n",
              "      <td>0.702884</td>\n",
              "      <td>0.687124</td>\n",
              "      <td>0.663653</td>\n",
              "      <td>-626.262817</td>\n",
              "      <td>93.897247</td>\n",
              "      <td>-0.691273</td>\n",
              "      <td>17.833763</td>\n",
              "      <td>9.502007</td>\n",
              "      <td>2.030928</td>\n",
              "      <td>-2.721135</td>\n",
              "      <td>-8.514406</td>\n",
              "      <td>-12.427499</td>\n",
              "      <td>-6.575863</td>\n",
              "      <td>-0.015912</td>\n",
              "      <td>-2.750585</td>\n",
              "      <td>0.777975</td>\n",
              "      <td>-5.365466</td>\n",
              "      <td>-0.337154</td>\n",
              "      <td>1.482861</td>\n",
              "      <td>-8.703282</td>\n",
              "      <td>-2.764846</td>\n",
              "      <td>-1.618086</td>\n",
              "      <td>-1.523441</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>0.008725</td>\n",
              "      <td>0.090577</td>\n",
              "      <td>0.060794</td>\n",
              "      <td>0.002684</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000047</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000055</td>\n",
              "      <td>0.000098</td>\n",
              "      <td>0.000262</td>\n",
              "      <td>0.000406</td>\n",
              "      <td>0.000398</td>\n",
              "      <td>0.000671</td>\n",
              "      <td>0.000650</td>\n",
              "      <td>0.000318</td>\n",
              "      <td>0.000125</td>\n",
              "      <td>0.000135</td>\n",
              "      <td>0.000105</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>0.000109</td>\n",
              "      <td>0.000126</td>\n",
              "      <td>0.000105</td>\n",
              "      <td>0.000053</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000028</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000032</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>8.432723e-07</td>\n",
              "      <td>calm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.195466</td>\n",
              "      <td>0.628032</td>\n",
              "      <td>0.687169</td>\n",
              "      <td>0.651985</td>\n",
              "      <td>0.621273</td>\n",
              "      <td>0.604192</td>\n",
              "      <td>0.640623</td>\n",
              "      <td>0.626136</td>\n",
              "      <td>0.652430</td>\n",
              "      <td>0.685134</td>\n",
              "      <td>0.653014</td>\n",
              "      <td>0.649654</td>\n",
              "      <td>0.632400</td>\n",
              "      <td>-535.881226</td>\n",
              "      <td>82.281357</td>\n",
              "      <td>-9.010551</td>\n",
              "      <td>20.842283</td>\n",
              "      <td>5.421832</td>\n",
              "      <td>-3.754339</td>\n",
              "      <td>-10.541499</td>\n",
              "      <td>-13.465772</td>\n",
              "      <td>-27.917681</td>\n",
              "      <td>-6.894572</td>\n",
              "      <td>-3.809465</td>\n",
              "      <td>-10.429282</td>\n",
              "      <td>0.157545</td>\n",
              "      <td>-7.953777</td>\n",
              "      <td>-6.011678</td>\n",
              "      <td>2.456674</td>\n",
              "      <td>-10.448029</td>\n",
              "      <td>-6.485257</td>\n",
              "      <td>-4.687830</td>\n",
              "      <td>-3.553447</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.000490</td>\n",
              "      <td>0.016251</td>\n",
              "      <td>0.110550</td>\n",
              "      <td>0.186236</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000099</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.000204</td>\n",
              "      <td>0.000287</td>\n",
              "      <td>0.000270</td>\n",
              "      <td>0.000436</td>\n",
              "      <td>0.000757</td>\n",
              "      <td>0.000782</td>\n",
              "      <td>0.000723</td>\n",
              "      <td>0.000844</td>\n",
              "      <td>0.000456</td>\n",
              "      <td>0.000389</td>\n",
              "      <td>0.000483</td>\n",
              "      <td>0.000514</td>\n",
              "      <td>0.000573</td>\n",
              "      <td>0.000368</td>\n",
              "      <td>0.000192</td>\n",
              "      <td>0.000170</td>\n",
              "      <td>0.000132</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>0.000055</td>\n",
              "      <td>0.000057</td>\n",
              "      <td>0.000076</td>\n",
              "      <td>0.000062</td>\n",
              "      <td>0.000061</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000052</td>\n",
              "      <td>0.000076</td>\n",
              "      <td>0.000086</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.000061</td>\n",
              "      <td>0.000052</td>\n",
              "      <td>0.000059</td>\n",
              "      <td>0.000095</td>\n",
              "      <td>0.000090</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>2.326331e-06</td>\n",
              "      <td>sad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.173769</td>\n",
              "      <td>0.720864</td>\n",
              "      <td>0.685492</td>\n",
              "      <td>0.655122</td>\n",
              "      <td>0.652557</td>\n",
              "      <td>0.587786</td>\n",
              "      <td>0.550012</td>\n",
              "      <td>0.638170</td>\n",
              "      <td>0.707171</td>\n",
              "      <td>0.648498</td>\n",
              "      <td>0.604207</td>\n",
              "      <td>0.638241</td>\n",
              "      <td>0.707306</td>\n",
              "      <td>-526.520569</td>\n",
              "      <td>84.466164</td>\n",
              "      <td>-6.822329</td>\n",
              "      <td>22.756920</td>\n",
              "      <td>8.021371</td>\n",
              "      <td>-0.836710</td>\n",
              "      <td>-6.375116</td>\n",
              "      <td>-13.950517</td>\n",
              "      <td>-15.801805</td>\n",
              "      <td>-1.701238</td>\n",
              "      <td>-3.240356</td>\n",
              "      <td>-2.120920</td>\n",
              "      <td>-1.001574</td>\n",
              "      <td>-5.576652</td>\n",
              "      <td>-0.277861</td>\n",
              "      <td>0.180505</td>\n",
              "      <td>-5.214784</td>\n",
              "      <td>-4.889361</td>\n",
              "      <td>-1.206443</td>\n",
              "      <td>2.497521</td>\n",
              "      <td>0.000392</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.001436</td>\n",
              "      <td>0.052773</td>\n",
              "      <td>0.284222</td>\n",
              "      <td>0.078999</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>0.000183</td>\n",
              "      <td>0.000524</td>\n",
              "      <td>0.000483</td>\n",
              "      <td>0.000608</td>\n",
              "      <td>0.000806</td>\n",
              "      <td>0.001164</td>\n",
              "      <td>0.001016</td>\n",
              "      <td>0.001356</td>\n",
              "      <td>0.000967</td>\n",
              "      <td>0.000642</td>\n",
              "      <td>0.000433</td>\n",
              "      <td>0.000357</td>\n",
              "      <td>0.000369</td>\n",
              "      <td>0.000310</td>\n",
              "      <td>0.000320</td>\n",
              "      <td>0.000237</td>\n",
              "      <td>0.000182</td>\n",
              "      <td>0.000108</td>\n",
              "      <td>0.000086</td>\n",
              "      <td>0.000088</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>0.000059</td>\n",
              "      <td>0.000119</td>\n",
              "      <td>0.000216</td>\n",
              "      <td>0.000222</td>\n",
              "      <td>0.000176</td>\n",
              "      <td>0.000146</td>\n",
              "      <td>0.000119</td>\n",
              "      <td>0.000097</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>0.000129</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.000243</td>\n",
              "      <td>0.000190</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>4.691918e-06</td>\n",
              "      <td>fear</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.207284</td>\n",
              "      <td>0.692981</td>\n",
              "      <td>0.737456</td>\n",
              "      <td>0.726056</td>\n",
              "      <td>0.685032</td>\n",
              "      <td>0.636497</td>\n",
              "      <td>0.568223</td>\n",
              "      <td>0.528898</td>\n",
              "      <td>0.598124</td>\n",
              "      <td>0.635435</td>\n",
              "      <td>0.643268</td>\n",
              "      <td>0.671737</td>\n",
              "      <td>0.665797</td>\n",
              "      <td>-591.298523</td>\n",
              "      <td>92.935883</td>\n",
              "      <td>-4.376369</td>\n",
              "      <td>22.136271</td>\n",
              "      <td>9.728477</td>\n",
              "      <td>-3.868228</td>\n",
              "      <td>-4.231765</td>\n",
              "      <td>-12.517565</td>\n",
              "      <td>-17.417633</td>\n",
              "      <td>-6.273466</td>\n",
              "      <td>-7.159021</td>\n",
              "      <td>-2.124696</td>\n",
              "      <td>-2.085358</td>\n",
              "      <td>-9.489192</td>\n",
              "      <td>-3.802913</td>\n",
              "      <td>-1.608241</td>\n",
              "      <td>-9.055273</td>\n",
              "      <td>-6.693238</td>\n",
              "      <td>-5.338201</td>\n",
              "      <td>-0.922801</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000040</td>\n",
              "      <td>0.001787</td>\n",
              "      <td>0.018344</td>\n",
              "      <td>0.063987</td>\n",
              "      <td>0.039720</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000155</td>\n",
              "      <td>0.000217</td>\n",
              "      <td>0.000380</td>\n",
              "      <td>0.000631</td>\n",
              "      <td>0.000542</td>\n",
              "      <td>0.000424</td>\n",
              "      <td>0.000528</td>\n",
              "      <td>0.000116</td>\n",
              "      <td>0.000108</td>\n",
              "      <td>0.000090</td>\n",
              "      <td>0.000116</td>\n",
              "      <td>0.000120</td>\n",
              "      <td>0.000075</td>\n",
              "      <td>0.000052</td>\n",
              "      <td>0.000053</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>4.218449e-07</td>\n",
              "      <td>sad</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 163 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0         0         1  ...       159           160  labels\n",
              "0           0  0.272267  0.689451  ...  0.000143  1.498768e-05   angry\n",
              "1           1  0.262035  0.603476  ...  0.000011  8.432723e-07    calm\n",
              "2           2  0.195466  0.628032  ...  0.000031  2.326331e-06     sad\n",
              "3           3  0.173769  0.720864  ...  0.000074  4.691918e-06    fear\n",
              "4           4  0.207284  0.692981  ...  0.000008  4.218449e-07     sad\n",
              "\n",
              "[5 rows x 163 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COcIH-7CJZgy"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = Features.iloc[: ,:-1].values\n",
        "y = Features['labels'].values\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "h,w = X.shape\n",
        "X = X.reshape(h,w,1)\n",
        "h = y.shape[0]\n",
        "y = y.reshape(h,1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2rNpHT8HU0f",
        "outputId": "bc9bf9d7-fbdb-4815-c146-d1af546023ee"
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1440, 162, 1), (1440, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "083UmV0wGsbz"
      },
      "source": [
        "#### To be changed with data not random distribution\n",
        "# encoder = OneHotEncoder()\n",
        "# y = encoder.fit_transform(np.array(y).reshape(-1,1)).toarray()\n",
        "\n",
        "# y.shape\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=0, shuffle=False)\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "# X_test = scaler.transform(X_test)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_j2iJM_Ji2f",
        "outputId": "58f34125-e9cf-4477-f3a4-23ff0ed60e94"
      },
      "source": [
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1152, 162, 1), (1152, 1), (288, 162, 1), (288, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0rbJwbcNlLM"
      },
      "source": [
        "X_train = tf.expand_dims(X_train, axis=-1)\n",
        "y_train = tf.expand_dims(y_train, axis=-1)\n",
        "X_test = tf.expand_dims(X_test, axis=-1)\n",
        "y_test = tf.expand_dims(y_test, axis=-1)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cN4ZdCEVGojS"
      },
      "source": [
        "def plotter(history):\n",
        "  plt.figure()\n",
        "  plt.plot(history.history['loss'],label='train loss')\n",
        "  plt.plot(history.history['val_loss'],label='test loss')\n",
        "  plt.xlabel('iterations')\n",
        "  plt.ylabel('losses')\n",
        "  plt.legend()\n",
        "  plt.figure()\n",
        "  plt.plot(history.history['accuracy'],label='train accuracy')\n",
        "  plt.plot(history.history['val_accuracy'],label='test accuracy')\n",
        "  plt.xlabel('iterations')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.legend()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s47IPH-MGt-F"
      },
      "source": [
        "#building CNN model \n",
        "from keras import models\n",
        "from keras import layers\n",
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "import tensorflow as tf\n",
        "\n",
        "config = ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = InteractiveSession(config=config)\n",
        "\n",
        "\n",
        "input_shape=[162, 162, 1]\n",
        "model = models.Sequential([\n",
        "                            tf.keras.layers.Conv2D(128, 3, activation=tf.nn.relu, input_shape=input_shape),\n",
        "                            tf.keras.layers.MaxPooling2D(2),\n",
        "                            tf.keras.layers.Dropout(0.2),\n",
        "                            tf.keras.layers.Conv2D(64, 3, activation=tf.nn.relu),\n",
        "                            tf.keras.layers.MaxPooling2D(2),\n",
        "                            tf.keras.layers.Dropout(0.2),\n",
        "                            tf.keras.layers.Conv2D(64, 3, activation=tf.nn.relu),\n",
        "                            tf.keras.layers.MaxPooling2D(2),\n",
        "                            tf.keras.layers.Dropout(0.2),\n",
        "                            tf.keras.layers.Conv2D(32, 3, activation=tf.nn.relu),\n",
        "                            tf.keras.layers.MaxPooling2D(2),\n",
        "                            tf.keras.layers.Dropout(0.2),\n",
        "                           \n",
        "                            tf.keras.layers.Flatten(),\n",
        "                            tf.keras.layers.Dense(y_train.shape[1], activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "optimizer = tf.optimizers.Adam();\n",
        "model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG_P11qyK5lb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ec339f6a-132d-4ee9-ed50-1f54952a61bb"
      },
      "source": [
        "history = model.fit(X_train,y_train,batch_size=256,epochs = 1000,verbose=1,validation_data=(X_test,y_test))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-f81bee5b1a27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1156\u001b[0m                 _r=1):\n\u001b[1;32m   1157\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    763\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 764\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3050\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3051\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3444\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3287\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3288\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3289\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:830 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:813 run_step  *\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:770 train_step  *\n        y_pred = self(x, training=True)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py:1006 __call__  *\n        outputs = call_fn(inputs, *args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:389 call  *\n        outputs = layer(inputs, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py:1006 __call__  *\n        outputs = call_fn(inputs, *args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1442 call  *\n        return getattr(self._module, self._method_name)(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:1030 __call__  **\n        outputs = call_fn(inputs, *args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/convolutional.py:249 call\n        outputs = self._convolution_op(inputs, self.kernel)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py:1019 convolution_v2\n        name=name)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py:1149 convolution_internal\n        name=name)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py:2614 _conv2d_expanded_batch\n        name=name)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py:313 squeeze_batch_dims\n        out_reshaped = op(inp_reshaped)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py:973 conv2d\n        data_format=data_format, dilations=dilations, name=name)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:750 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py:601 _create_op_internal\n        compute_device)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:3565 _create_op_internal\n        op_def=op_def)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:2042 __init__\n        control_input_ops, op_def)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:1883 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Negative dimension size caused by subtracting 3 from 1 for '{{node sequential_16/module_wrapper_215/conv2d_61/Conv2D/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](sequential_16/module_wrapper_215/conv2d_61/Conv2D/Reshape, sequential_16/module_wrapper_215/conv2d_61/Conv2D/Conv2D/ReadVariableOp)' with input shapes: [?,1,1,1], [3,3,1,128].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMJlhjEXK8Ax"
      },
      "source": [
        "plotter(history)\n",
        "\n",
        "model.summary()\n",
        "result = model.evaluate(X_test,y_test)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}